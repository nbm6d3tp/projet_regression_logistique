{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#**Projet - Régression logistique**\n",
        "\n",
        "But : implémenter la régression logistique et mettre en oeuvre une méthodologie solide\n",
        "\n",
        "Cours : M1 MIAGE apprentissage (Dauphine) - 2023-2024\n",
        "\n",
        "Projet en binôme\n",
        "\n",
        "A rendre :\n",
        "*   Code ayant permis de résoudre les différentes questions (Notebook Python) - commenté pour permettre son exécution\n",
        "*   Rapport en PDF (moins de 20 pages)\n",
        "\n",
        "Date : le 01/12/2023 (avant 20h - heure de Paris)"
      ],
      "metadata": {
        "id": "rlaBr1gIo4cy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# libairies utiles pour ce cours\n",
        "\n",
        "import sklearn as sk\n",
        "from sklearn import datasets, model_selection\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn import tree\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.inspection import DecisionBoundaryDisplay\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "import copy\n",
        "import pandas as pd\n",
        "from scipy import stats\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.colors import ListedColormap"
      ],
      "metadata": {
        "id": "FMR5OA_8o4Dh"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## I Regression logistique (8pts)\n",
        "\n",
        "1.   En vous appuyant sur le code écrit pour le TP 2 (notamment la descente de gradient), implémenter la régression logistique (2pts)\n",
        "\n",
        "  /!\\ /!\\  *il faudra modifier le code ou le problème d'optimisation car dans le TP2, on minimisait une fonction et là, on maximise une fonction* /!\\ /!\\\n",
        "2. Calculer $\\beta$, les paramètres de la frontière de décision (1 pt)\n",
        "8. prédire la classe du point $x = \\left( \\begin{array}{c}\n",
        "0 \\\\\n",
        "0 \\end{array} \\right)$ (1 pt)\n",
        "9. Représenter la frontière de décision (en ré-utilisant le code écrit pour le TP 1) (1pt)\n",
        "10. Développer une fonction de calcul de taux d'erreur d'un modèle. Appliquer ce code pour estimer le taux d'erreur (en apprentissage et en test) de votre modèle de régression logistique. (1pt)\n",
        "11. Comparer ces résultats (frontière de décisions et taux d'erreurs) à ceux obtenus sur ces données par votre code de LDA (développé pour le TP 1) - commenter (2 pts)"
      ],
      "metadata": {
        "id": "2IugSvnwpgzr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## II Méthodologie (5pts)\n",
        "\n",
        "7. inclure une ligne permettant d'ajouter un outlier dans les données d'apprentissage (ligne commentée dans le bloc de génération de données).\n",
        "Comparez les résultats de vos codes de régression logistique et de LDA sur ces données. Commenter (1pt)\n",
        "8. en plus des taux d'erreurs en apprentissage et en test, appliquer une statégie de validation croisée à n blocs et reporter les taux d'erreur (moyenne et variance) des deux modèles. (2pts)\n",
        "9. modifier le code de génération de données (en augmentant le nombre de dimensions et en ajoutant des variables correlées). Comparer le comportement des deux modèles (2pts)      *(on commentera à nouveau la ligne de génération d'outlier pour cette question)*\n",
        "\n"
      ],
      "metadata": {
        "id": "7LFvPx9lq07i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## III A vous de jouer (7 pts)\n",
        "10. Trouver/Générer/Récolter des données adéquates pour l'application d'un algorithme de classification binaire. Décrire ces données (origine, nombre et types de variables, analyse statistique des variables, nombre d'observations, contexte,...) (2pts)\n",
        "11. Appliquer vos codes de régression logistique et de LDA et reporter les performances obtenues (en apprentissage, en test ou en validation croisées) et commenter (1pts)\n",
        "12. Trouver une autre méthode à laquelle comparer les deux codes que vous avez développés, décrire (brièvement) le fonctionnement de cette troisème approche et l'appliquer sur les données (de la question 10). On pourra développer soit-même le code ou utiliser une librairie existante. Comparer et discuter les performances obtenues (4pts)"
      ],
      "metadata": {
        "id": "wdA6AGjNrUQL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def statistic_data(X,y):\n",
        "  print(\"Number of observations: \",X.shape[0])\n",
        "  print(\"Number of features: \",X.shape[1])\n",
        "  stat_0, p_value_0 = stats.normaltest(X[y==0]) #Tester si la distribution de X des observations de classe 0 est approximativement normale\n",
        "  stat_1, p_value_1 = stats.normaltest(X[y==1]) #Tester si la distribution de X des observations de classe 0 est approximativement normale\n",
        "  print(\"Is distribution of X approximately normal in class 0: \",p_value_0>= 0.05)\n",
        "  print(\"Is distribution of X approximately normal in class 1: \",p_value_1>= 0.05)\n",
        "\n",
        "def calculate_y_hat_logistic_regression(beta, Xb):\n",
        "  return 1/(1+np.exp(-np.dot(Xb,beta)))\n",
        "\n",
        "def calculate_y_hat_using_LDA (w,b,X):\n",
        "  return X@w+b\n",
        "\n",
        "# I.1\n",
        "def gradient_ascent(Xb,y):\n",
        "  itermax=10000 # nombre d'itérations maximum\n",
        "  i = 1\n",
        "  notconv=True #critere de convergence\n",
        "\n",
        "  eta=1e-3 #pas de la ascension de gradient\n",
        "\n",
        "  beta=np.random.normal(size=(Xb.shape[1],1)) #initialisation aléatoire de beta\n",
        "\n",
        "  losses=[] #calcul de la fonction cout à chaque iteration\n",
        "  loss_old = np.inf\n",
        "  y = y.reshape(-1,1)\n",
        "  reason=\"attain itermax\"\n",
        "  while (i <= itermax) and (notconv):\n",
        "    beta_old=beta\n",
        "\n",
        "    y_hat=calculate_y_hat_logistic_regression(beta, Xb)\n",
        "    grad=Xb.T@(y-y_hat)\n",
        "    beta=beta_old + eta * grad\n",
        "\n",
        "    loss=-np.mean((y * np.log(y_hat) + (1 - y) * np.log(1 - y_hat)))\n",
        "    losses.append(loss)\n",
        "\n",
        "    if np.linalg.norm(beta_old-beta)<1e-10: # critère d'arrêt 1\n",
        "      notconv=False\n",
        "      reason=\"diff iterate\"\n",
        "\n",
        "    if loss>loss_old: # critère d'arrêt 2\n",
        "      notconv=False\n",
        "      reason = \"falling loss\"\n",
        "\n",
        "    i += 1\n",
        "    loss_old=loss\n",
        "  return beta,losses, reason\n",
        "\n",
        "def LDA(X_train,y_train):\n",
        "  mu_0 = np.mean(X_train[y_train==0,:],axis=0)\n",
        "  mu_1 = np.mean(X_train[y_train==1,:],axis=0)\n",
        "\n",
        "  pi_0 = np.mean(y_train==0)\n",
        "  pi_1 = np.mean(y_train==1)\n",
        "\n",
        "  Xtemp = np.concatenate((X_train[y_train==0,:]-mu_0,X_train[y_train==1,:]-mu_1),axis=0)\n",
        "  Sigma = (Xtemp.T)@Xtemp/X_train.shape[0]\n",
        "  iSigma= np.linalg.inv(Sigma)\n",
        "  w = iSigma@(mu_0-mu_1)\n",
        "  b = -1/2 * (mu_0-mu_1).T@iSigma@(mu_0+mu_1) + np.log(pi_0/pi_1)\n",
        "  return w,b\n",
        "\n",
        "def draw_decision_boundary_config(X,y):\n",
        "  x1_min, x1_max = X[:, 0].min() - 0.5, X[:, 0].max() + 0.5\n",
        "  x2_min, x2_max = X[:, 1].min() - 0.5, X[:, 1].max() + 0.5\n",
        "  h = .02\n",
        "\n",
        "  x1_axis = np.arange(x1_min, x1_max, h)\n",
        "  x2_axis = np.arange(x2_min, x2_max, h)\n",
        "\n",
        "  x1_coors, x2_coors = np.meshgrid(x1_axis, x2_axis)\n",
        "\n",
        "  return x1_axis,x2_axis, x1_coors, x2_coors\n",
        "\n",
        "def draw_decision_boundary_logistic_regression(beta,X,y,xlabel,ylabel):\n",
        "  x1_axis,x2_axis, x1_coors, x2_coors=draw_decision_boundary_config(X,y)\n",
        "  big_x = np.column_stack((x1_coors.ravel(), x2_coors.ravel(), np.ones(x1_coors.ravel().shape[0])))\n",
        "\n",
        "  figure = plt.figure()\n",
        "  decision_boundary_coor = calculate_y_hat_logistic_regression(beta, big_x).reshape(x1_coors.shape)\n",
        "\n",
        "  cm_bright = ListedColormap(['#FF0000', '#0000FF'])\n",
        "  plt.pcolormesh(x1_axis, x2_axis, decision_boundary_coor>=0.5, cmap=cm_bright, shading='auto', alpha=0.6)\n",
        "  plt.scatter(X[:, 0], X[:, 1], c=y, cmap=cm_bright,edgecolors='k')\n",
        "  plt.title(\"Frontière de décision Régression Logistique\")\n",
        "  plt.xlabel(xlabel)\n",
        "  plt.ylabel(ylabel)\n",
        "\n",
        "def draw_decision_boundary_LDA(w,b,X,y,xlabel,ylabel):\n",
        "  x1_axis,x2_axis, x1_coors, x2_coors=draw_decision_boundary_config(X,y)\n",
        "  big_x = np.column_stack((x1_coors.ravel(), x2_coors.ravel()))\n",
        "\n",
        "  figure = plt.figure()\n",
        "  decision_boundary_coor =calculate_y_hat_using_LDA(w,b,big_x).reshape(x1_coors.shape)\n",
        "\n",
        "  cm_bright = ListedColormap(['#FF0000', '#0000FF'])\n",
        "  plt.pcolormesh(x1_axis, x2_axis, decision_boundary_coor<0, cmap=cm_bright, shading='auto', alpha=0.6)\n",
        "  plt.scatter(X[:, 0], X[:, 1], c=y, cmap=cm_bright,edgecolors='k')\n",
        "  plt.title(\"Frontière de décision LDA\")\n",
        "  plt.xlabel(xlabel)\n",
        "  plt.ylabel(ylabel)\n",
        "\n",
        "def draw_decision_boundary_decision_tree(clf,X,y,xlabel,ylabel):\n",
        "  x1_axis,x2_axis, x1_coors, x2_coors=draw_decision_boundary_config(X,y)\n",
        "  big_x = np.column_stack((x1_coors.ravel(), x2_coors.ravel()))\n",
        "\n",
        "  figure = plt.figure()\n",
        "  decision_boundary_coor =clf.predict(big_x).reshape(x1_coors.shape)\n",
        "\n",
        "  cm_bright = ListedColormap(['#FF0000', '#0000FF'])\n",
        "  plt.pcolormesh(x1_axis, x2_axis, decision_boundary_coor==1, cmap=cm_bright, shading='auto', alpha=0.6)\n",
        "  plt.scatter(X[:, 0], X[:, 1], c=y, cmap=cm_bright,edgecolors='k')\n",
        "  plt.title(\"Frontière de décision Decision Tree\")\n",
        "  plt.xlabel(xlabel)\n",
        "  plt.ylabel(ylabel)\n",
        "\n",
        "def calculate_logistic_regression_error_ratio(beta,Xb,y):\n",
        "  y_hat=calculate_y_hat_logistic_regression(beta, Xb)\n",
        "  y_hat[y_hat>=0.5]=1\n",
        "  y_hat[y_hat<0.5]=0\n",
        "  nb_correct_prediction=0\n",
        "  for i in range (0,len(y)):\n",
        "    if(y_hat[i]==y[i]):\n",
        "      nb_correct_prediction+=1\n",
        "  return (len(y)-nb_correct_prediction)/len(y)\n",
        "\n",
        "def calculate_LDA_error_ratio(w,b,X,y):\n",
        "  y_hat=calculate_y_hat_using_LDA(w,b,X)\n",
        "  y_hat[y_hat>=0]=0\n",
        "  y_hat[y_hat<0]=1\n",
        "  nb_correct_prediction=0\n",
        "  for i in range (0,len(y)):\n",
        "    if(y_hat[i]==y[i]):\n",
        "      nb_correct_prediction+=1\n",
        "  return (len(y)-nb_correct_prediction)/len(y)\n",
        "\n",
        "\n",
        "def calculate_decision_tree_error_ratio(clf,X,y):\n",
        "  y_hat=clf.predict(X)\n",
        "  nb_correct_prediction=0\n",
        "  for i in range (0,len(y)):\n",
        "    if(y_hat[i]==y[i]):\n",
        "      nb_correct_prediction+=1\n",
        "  return (len(y)-nb_correct_prediction)/len(y)\n",
        "\n",
        "def predict_class_of_point_using_logistic_regression(beta,xb):\n",
        "  y_hat = calculate_y_hat_logistic_regression(beta, xb)\n",
        "  if(y_hat>=0.5):\n",
        "    print(\"Logistic regression says that x belongs to class 1\")\n",
        "  else:\n",
        "    print(\"Logistic regression says that x belongs to class 0\")\n",
        "\n",
        "def predict_class_of_point_using_LDA(w,b,xb):\n",
        "  y_hat = calculate_y_hat_using_LDA(w,b,xb)\n",
        "  if(y_hat>=0):\n",
        "    print(\"LDA says that x belongs to class 0\")\n",
        "  else:\n",
        "    print(\"LDA says that x belongs to class 1\")\n",
        "\n",
        "def report_error_ratio(X,y):\n",
        "  loo = model_selection.LeaveOneOut()\n",
        "  errors_logistic_regression=[]\n",
        "  errors_LDA=[]\n",
        "  errors_decision_tree=[]\n",
        "\n",
        "  for train_index, test_index in loo.split(X):\n",
        "    X_train, X_test = X[train_index], X[test_index]\n",
        "    y_train, y_test = y[train_index], y[test_index]\n",
        "\n",
        "    Xb=np.column_stack((X_train,np.ones(X_train.shape[0])))\n",
        "    Xtb=np.column_stack((X_test,np.ones(X_test.shape[0])))\n",
        "\n",
        "    clf = DecisionTreeClassifier(random_state=0)\n",
        "    clf.fit(X_train, y_train)\n",
        "    errors_decision_tree.append(calculate_decision_tree_error_ratio(clf,X_test,y_test))\n",
        "\n",
        "    beta,_,_=gradient_ascent(Xb,y_train)\n",
        "    errors_logistic_regression.append(calculate_logistic_regression_error_ratio(beta,Xtb,y_test))\n",
        "    #Parfois, on se trouve dans un cas où matrice Sigma n'est pas inversible. Si c'est le cas, j'exclus le résultat de cette itération du tableau d'erreurs de LDA\n",
        "    #Ce problème est peut-être causé par:\n",
        "    #- Multicolinéarité : certaines variables sont fortement corrélées entre elles, la matrice de covariance peut devenir singulière, ce qui signifie qu'elle n'est pas inversible.\n",
        "    #- Données redondantes : Si les variables sont redondantes, c'est-à-dire qu'elles ne fournissent pas d'informations supplémentaires, cela peut conduire à une matrice de covariance singulière.\n",
        "    try:\n",
        "      w,b=LDA(X_train,y_train)\n",
        "      errors_LDA.append(calculate_LDA_error_ratio(w,b,X_test,y_test))\n",
        "    except:\n",
        "      continue\n",
        "\n",
        "  mean_error_logistic_regression=np.mean(errors_logistic_regression)\n",
        "  var_error_logistic_regression = np.var(errors_logistic_regression)\n",
        "\n",
        "  mean_error_LDA=np.mean(errors_LDA)\n",
        "  var_error_LDA = np.var(errors_LDA)\n",
        "\n",
        "  mean_error_decision_tree= np.mean(errors_decision_tree)\n",
        "  var_error_decision_tree = np.var(errors_decision_tree)\n",
        "\n",
        "  return mean_error_logistic_regression,var_error_logistic_regression,mean_error_LDA,var_error_LDA,mean_error_decision_tree,var_error_decision_tree"
      ],
      "metadata": {
        "id": "4U0UPTs9p7mW"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XN-Z3pZZor1_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6526dae5-cf9a-4b08-ca34-1e9548ba28d7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of observations:  100\n",
            "Number of features:  2\n",
            "Is distribution of X approximately normal in class 0:  [ True  True]\n",
            "Is distribution of X approximately normal in class 1:  [ True False]\n",
            "attain itermax\n",
            "beta= [[7.09905713]\n",
            " [2.83597819]\n",
            " [0.19270779]]\n",
            "w= [-7.30352402 -2.66269449] , b= 2.842390511575443\n",
            "Error ratio of model generated by LDA on training data:  0.0375\n",
            "Error ratio of model generated by LDA on testing data:  0.05\n",
            "Error ratio of model generated by Logistic Regression on training data:  0.025\n",
            "Error ratio of model generated by Logistic Regression on testing data:  0.05\n",
            "Error ratio of model generated by Decision Tree on training data:  0.0\n",
            "Error ratio of model generated by Decision Tree on testing data:  0.15\n",
            "Logistic regression says that x belongs to class 1\n",
            "LDA says that x belongs to class 0\n"
          ]
        }
      ],
      "source": [
        "def exerciceI():\n",
        "  #generation de données\n",
        "  n=100\n",
        "  rng = np.random.RandomState(2)\n",
        "  dim =2\n",
        "  X,y = sk.datasets.make_classification(n_samples=n, n_features=dim, n_redundant=0, n_informative=dim, n_repeated=0, n_clusters_per_class=1, random_state=rng)\n",
        "  statistic_data(X,y)\n",
        "  X_train, X_test, y_train, y_test = sk.model_selection.train_test_split(X, y, test_size=.2, random_state=42)\n",
        "  Xb=np.column_stack((X_train,np.ones(X_train.shape[0])))\n",
        "  Xtb=np.column_stack((X_test,np.ones(X_test.shape[0])))\n",
        "\n",
        "  figure = plt.figure()\n",
        "  cm_bright = ListedColormap(['#FF0000', '#0000FF'])\n",
        "  plt.scatter(X[:, 0], X[:, 1], c=y, cmap=cm_bright,edgecolors='k')\n",
        "  plt.title(\"Representation de données\")\n",
        "\n",
        "  #I.2: Calculer β , les paramètres de la frontière de décision\n",
        "  beta,losses,reason=gradient_ascent(Xb,y_train)\n",
        "  figure = plt.figure()\n",
        "  plt.plot(losses) # affichage de la perte (au fur et a mesure des des itérations)\n",
        "  plt.title(\"Pertes au fil du temp\")\n",
        "  print(reason)\n",
        "  print(\"beta=\",beta)\n",
        "\n",
        "  #I.6: Comparer avec LDA\n",
        "  w,b=LDA(X_train,y_train)\n",
        "  print(\"w=\",w,\", b=\",b)\n",
        "  print(\"Error ratio of model generated by LDA on training data: \",calculate_LDA_error_ratio(w,b,X_train,y_train))\n",
        "  print(\"Error ratio of model generated by LDA on testing data: \",calculate_LDA_error_ratio(w,b,X_test,y_test))\n",
        "\n",
        "      #I.5: Taux d'erreur du modèle Logistic Regression\n",
        "  print(\"Error ratio of model generated by Logistic Regression on training data: \",calculate_logistic_regression_error_ratio(beta,Xb,y_train))\n",
        "  print(\"Error ratio of model generated by Logistic Regression on testing data: \",calculate_logistic_regression_error_ratio(beta,Xtb,y_test))\n",
        "\n",
        "  clf = DecisionTreeClassifier(random_state=0)\n",
        "  clf.fit(X_train, y_train)\n",
        "  print(\"Error ratio of model generated by Decision Tree on training data: \",calculate_decision_tree_error_ratio(clf,X_train,y_train))\n",
        "  print(\"Error ratio of model generated by Decision Tree on testing data: \",calculate_decision_tree_error_ratio(clf,X_test,y_test))\n",
        "\n",
        "      #I.3: prédire la classe du point x [0,0]\n",
        "  x=[0,0]\n",
        "  xb=np.append(x,1)\n",
        "  predict_class_of_point_using_logistic_regression(beta,xb)\n",
        "  predict_class_of_point_using_LDA(w,b,x)\n",
        "\n",
        "  draw_decision_boundary_LDA(w,b,X,y,\"X1\",\"X2\")\n",
        "        #I.4: frontière de décision de régression logistique\n",
        "  draw_decision_boundary_logistic_regression(beta,X,y,\"X1\",\"X2\")\n",
        "\n",
        "  mean_error_logistic_regression,var_error_logistic_regression,mean_error_LDA,var_error_LDA,mean_error_decision_tree,var_error_decision_tree=report_error_ratio(X,y)\n",
        "  print(\"Mean errors Logistic Regression\", mean_error_logistic_regression)\n",
        "  print(\"Var errors Logistic Regression\", var_error_logistic_regression)\n",
        "  print(\"Mean errors LDA\", mean_error_LDA)\n",
        "  print(\"Var errors LDA\", var_error_LDA)\n",
        "  print(\"Mean errors Decision Tree\", mean_error_decision_tree)\n",
        "  print(\"Var errors Decision Tree\", var_error_decision_tree)\n",
        "\n",
        "exerciceI()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def exerciceII():\n",
        "  #generation de données\n",
        "  n=100\n",
        "  rng = np.random.RandomState(2)\n",
        "  dim =2\n",
        "  X,y = sk.datasets.make_classification(n_samples=n, n_features=dim, n_redundant=0, n_informative=dim, n_repeated=0, n_clusters_per_class=1, random_state=rng)\n",
        "  statistic_data(X,y)\n",
        "  X_train, X_test, y_train, y_test = sk.model_selection.train_test_split(X, y, test_size=.2, random_state=42)\n",
        "  Xb=np.column_stack((X_train,np.ones(X_train.shape[0])))\n",
        "  Xtb=np.column_stack((X_test,np.ones(X_test.shape[0])))\n",
        "\n",
        "  # inclure cette ligne pour la question II.7\n",
        "  # II.7: Comparez les résultats de vos codes de régression logistique et de LDA sur ces données\n",
        "  X_new_train=copy.deepcopy(X_train)\n",
        "  X_new_train[0,:] = [5,5]\n",
        "  Xb_new=np.column_stack((X_new_train,np.ones(X_new_train.shape[0])))\n",
        "\n",
        "  beta,losses,reason=gradient_ascent(Xb_new,y_train)\n",
        "  w,b=LDA(X_new_train,y_train)\n",
        "  print(\"new beta=\",beta)\n",
        "  print(reason)\n",
        "  plt.plot(losses) # affichage de la perte (au fur et a mesure des des itérations)\n",
        "\n",
        "  print(\"new w=\",w,\", new b=\",b)\n",
        "  print(\"Error ratio of new model generated by LDA on new training data: \",calculate_LDA_error_ratio(w,b,X_new_train,y_train))\n",
        "  print(\"Error ratio of new model generated by LDA on testing data: \",calculate_LDA_error_ratio(w,b,X_test,y_test))\n",
        "\n",
        "  print(\"Error ratio of new model generated by Logistic Regression on new training data: \",calculate_logistic_regression_error_ratio(beta,Xb_new,y_train))\n",
        "  print(\"Error ratio of new model generated by Logistic Regression on new testing data: \",calculate_logistic_regression_error_ratio(beta,Xtb,y_test))\n",
        "\n",
        "  clf = DecisionTreeClassifier(random_state=0)\n",
        "  clf.fit(X_train, y_train)\n",
        "  print(\"Error ratio of model generated by Decision Tree on new training data: \",calculate_decision_tree_error_ratio(clf,X_train,y_train))\n",
        "  print(\"Error ratio of model generated by Decision Tree on new testing data: \",calculate_decision_tree_error_ratio(clf,X_test,y_test))\n",
        "\n",
        "  x=[0,0]\n",
        "  xb=np.append(x,1)\n",
        "  predict_class_of_point_using_logistic_regression(beta,xb)\n",
        "  predict_class_of_point_using_LDA(w,b,x)\n",
        "\n",
        "  new_X= np.concatenate((X_new_train, X_test), axis=0)\n",
        "  new_y= np.concatenate((y_train, y_test), axis=0)\n",
        "  draw_decision_boundary_LDA(w,b,new_X,new_y,\"X1\",\"X2\")\n",
        "  draw_decision_boundary_logistic_regression(beta,new_X,new_y,\"X1\",\"X2\")\n",
        "\n",
        "  #II.8: Stratégie de validation croisée à n blocs et reporter les taux d'erreur (moyenne et variance) des deux modèles.\n",
        "  mean_error_logistic_regression,var_error_logistic_regression,mean_error_LDA,var_error_LDA,mean_error_decision_tree,var_error_decision_tree=report_error_ratio(X,y)\n",
        "  print(\"Mean errors Logistic Regression\", mean_error_logistic_regression)\n",
        "  print(\"Var errors Logistic Regression\", var_error_logistic_regression)\n",
        "  print(\"Mean errors LDA\", mean_error_LDA)\n",
        "  print(\"Var errors LDA\", var_error_LDA)\n",
        "  print(\"Mean errors Decision Tree\", mean_error_decision_tree)\n",
        "  print(\"Var errors Decision Tree\", var_error_decision_tree)\n",
        "\n",
        "exerciceII()"
      ],
      "metadata": {
        "id": "Pt8b4lAtlOGh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def exerciceII9():\n",
        "  #II.9: Modifier le code de génération de données (en augmentant le nombre de dimensions et en ajoutant des variables correlées). Comparer le comportement des deux modèles\n",
        "  n=100\n",
        "  rng = np.random.RandomState(2)\n",
        "  dim=7\n",
        "  X, y = sk.datasets.make_classification(\n",
        "      n_samples=n,\n",
        "      n_features=dim,\n",
        "      n_redundant=2,\n",
        "      n_informative=2,\n",
        "      n_clusters_per_class=1,\n",
        "      random_state=rng\n",
        "  )\n",
        "  statistic_data(X,y)\n",
        "\n",
        "  #pour pouvoir représenter les données sous forme de 2D, j'ai réduit le nombre d'attributs en utilisant PCA\n",
        "  #(Suite à ce que je connais, on pourra utiliser w généné par LDA normalement pour faire ça aussi, mais faute de temps, je n'ai pas pu faire des recherches en détail sur ça.\n",
        "  #Donc, j'ai utilisé une fonction qui déjà existe dans la librairie sklearn pour la sûreté)\n",
        "  pca = PCA(n_components=2)\n",
        "  transformed_X = pca.fit_transform(X,y)\n",
        "  transformed_Xb = np.column_stack((transformed_X,np.ones(transformed_X.shape[0])))\n",
        "\n",
        "  X_train, X_test, y_train, y_test = sk.model_selection.train_test_split(X, y, test_size=.2, random_state=42)\n",
        "\n",
        "  Xb=np.column_stack((X_train,np.ones(X_train.shape[0])))\n",
        "  Xtb=np.column_stack((X_test,np.ones(X_test.shape[0])))\n",
        "\n",
        "  figure = plt.figure()\n",
        "  cm_bright = ListedColormap(['#FF0000', '#0000FF'])\n",
        "  plt.scatter(transformed_X[:, 0], transformed_X[:, 1], c=y, cmap=cm_bright,edgecolors='k')\n",
        "  plt.show()\n",
        "\n",
        "  beta,losses,reason=gradient_ascent(Xb,y_train)\n",
        "  w,b=LDA(X_train,y_train)\n",
        "\n",
        "  #Les paramètres de 2 modèles sur les données réduites par PCA, sertent à représenter les frontières de décision en 2D\n",
        "  transformed_beta,_,_=gradient_ascent(transformed_Xb,y)\n",
        "  transformed_w,transformed_b=LDA(transformed_X,y)\n",
        "\n",
        "  print(\"beta=\",beta)\n",
        "  print(reason)\n",
        "  figure = plt.figure()\n",
        "  plt.plot(losses) # affichage de la perte (au fur et a mesure des des itérations)\n",
        "\n",
        "  print(\"w=\",w,\", b=\",b)\n",
        "  print(\"Error ratio of new model generated by LDA on training data: \",calculate_LDA_error_ratio(w,b,X_train,y_train))\n",
        "  print(\"Error ratio of new model generated by LDA on testing data: \",calculate_LDA_error_ratio(w,b,X_test,y_test))\n",
        "\n",
        "  print(\"Error ratio of new model generated by Logistic Regression on training data: \",calculate_logistic_regression_error_ratio(beta,Xb,y_train))\n",
        "  print(\"Error ratio of new model generated by Logistic Regression on testing data: \",calculate_logistic_regression_error_ratio(beta,Xtb,y_test))\n",
        "\n",
        "  clf = DecisionTreeClassifier(random_state=0)\n",
        "  clf.fit(X_train, y_train)\n",
        "  print(\"Error ratio of model generated by Decision Tree on training data: \",calculate_decision_tree_error_ratio(clf,X_train,y_train))\n",
        "  print(\"Error ratio of model generated by Decision Tree on testing data: \",calculate_decision_tree_error_ratio(clf,X_test,y_test))\n",
        "\n",
        "  draw_decision_boundary_LDA(transformed_w,transformed_b,transformed_X,y,\"PC1\",\"PC2\")\n",
        "  draw_decision_boundary_logistic_regression(transformed_beta,transformed_X,y,\"PC1\",\"PC2\")\n",
        "\n",
        "  stat_0, p_value_0 = stats.normaltest(X[y==0])\n",
        "  stat_1, p_value_1 = stats.normaltest(X[y==1])\n",
        "  print(p_value_0>= 0.05)\n",
        "  print(p_value_1>= 0.05)\n",
        "\n",
        "  mean_error_logistic_regression,var_error_logistic_regression,mean_error_LDA,var_error_LDA,mean_error_decision_tree,var_error_decision_tree=report_error_ratio(X,y)\n",
        "  print(\"Mean errors Logistic Regression\", mean_error_logistic_regression)\n",
        "  print(\"Var errors Logistic Regression\", var_error_logistic_regression)\n",
        "  print(\"Mean errors LDA\", mean_error_LDA)\n",
        "  print(\"Var errors LDA\", var_error_LDA)\n",
        "  print(\"Mean errors Decision Tree\", mean_error_decision_tree)\n",
        "  print(\"Var errors Decision Tree\", var_error_decision_tree)\n",
        "\n",
        "exerciceII9()"
      ],
      "metadata": {
        "id": "69Qya-LRKyAP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def exerciceIII():\n",
        "  scaler = StandardScaler()\n",
        "\n",
        "  #génération et traitement de données. Choisir que les colonnes ayant values de type \"number\". Remplacer toutes les valeurs NaN avec la moyenne de leurs colonne.\n",
        "  df = pd.read_csv('./heart.csv')\n",
        "  df = df.select_dtypes(include=np.number)\n",
        "  df = df.fillna(df.mean())\n",
        "  dfy=df['target']\n",
        "  dfx=df.drop(\"target\", axis=1)\n",
        "  X = scaler.fit_transform(dfx)\n",
        "  y=dfy.to_numpy()\n",
        "  statistic_data(X,y)\n",
        "\n",
        "  pca = PCA(n_components=2)\n",
        "  transformed_X = pca.fit_transform(X,y)\n",
        "  transformed_Xb = np.column_stack((transformed_X,np.ones(transformed_X.shape[0])))\n",
        "\n",
        "  X_train, X_test, y_train, y_test = sk.model_selection.train_test_split(X, y, test_size=.2, random_state=42)\n",
        "\n",
        "  Xb=np.column_stack((X_train,np.ones(X_train.shape[0])))\n",
        "  Xtb=np.column_stack((X_test,np.ones(X_test.shape[0])))\n",
        "\n",
        "  figure = plt.figure()\n",
        "  cm_bright = ListedColormap(['#FF0000', '#0000FF'])\n",
        "  plt.scatter(transformed_X[:, 0], transformed_X[:, 1], c=y, cmap=cm_bright,edgecolors='k')\n",
        "  plt.show()\n",
        "\n",
        "  beta,losses,reason=gradient_ascent(Xb,y_train)\n",
        "  w,b=LDA(X_train,y_train)\n",
        "  clf = DecisionTreeClassifier(random_state=0)\n",
        "  clf.fit(X_train, y_train)\n",
        "\n",
        "  transformed_beta,_,_=gradient_ascent(transformed_Xb,y)\n",
        "  transformed_w,transformed_b=LDA(transformed_X,y)\n",
        "  transformed_clf = DecisionTreeClassifier(random_state=0)\n",
        "  transformed_clf.fit(transformed_X, y)\n",
        "\n",
        "  figure = plt.figure()\n",
        "  plt.plot(losses) # affichage de la perte (au fur et a mesure des des itérations)\n",
        "  print(reason)\n",
        "  print(\"beta=\",beta)\n",
        "\n",
        "  print(\"w=\",w,\", b=\",b)\n",
        "  print(\"Error ratio of new model generated by LDA on training data: \",calculate_LDA_error_ratio(w,b,X_train,y_train))\n",
        "  print(\"Error ratio of new model generated by LDA on testing data: \",calculate_LDA_error_ratio(w,b,X_test,y_test))\n",
        "\n",
        "  print(\"Error ratio of new model generated by Logistic Regression on training data: \",calculate_logistic_regression_error_ratio(beta,Xb,y_train))\n",
        "  print(\"Error ratio of new model generated by Logistic Regression on testing data: \",calculate_logistic_regression_error_ratio(beta,Xtb,y_test))\n",
        "\n",
        "  print(\"Error ratio of new model generated by Decision Tree on training data: \",calculate_decision_tree_error_ratio(clf,X_train,y_train))\n",
        "  print(\"Error ratio of new model generated by Decision Tree on testing data: \",calculate_decision_tree_error_ratio(clf,X_test,y_test))\n",
        "\n",
        "  draw_decision_boundary_LDA(transformed_w,transformed_b,transformed_X,y,\"PC1\",\"PC2\")\n",
        "  draw_decision_boundary_logistic_regression(transformed_beta,transformed_X,y,\"PC1\",\"PC2\")\n",
        "  draw_decision_boundary_decision_tree(transformed_clf,transformed_X,y,\"PC1\",\"PC2\")\n",
        "\n",
        "  mean_error_logistic_regression,var_error_logistic_regression,mean_error_LDA,var_error_LDA,mean_error_decision_tree,var_error_decision_tree=report_error_ratio(X,y)\n",
        "\n",
        "  print(\"Mean errors Logistic Regression\", mean_error_logistic_regression)\n",
        "  print(\"Var errors Logistic Regression\", var_error_logistic_regression)\n",
        "  print(\"Mean errors LDA\", mean_error_LDA)\n",
        "  print(\"Var errors LDA\", var_error_LDA)\n",
        "  print(\"Mean errors Decision Tree\", mean_error_decision_tree)\n",
        "  print(\"Var errors Decision Tree\", var_error_decision_tree)\n",
        "\n",
        "exerciceIII()"
      ],
      "metadata": {
        "id": "ragNdZ9hVwFv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}